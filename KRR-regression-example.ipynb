{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367eb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to set some defaults for nice plots\n",
    "import matplotlib.pyplot as plt\n",
    "MEDIUM_SIZE = 12\n",
    "plt.rcParams[\"figure.figsize\"] = (12,4) # (w, h)\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## normally we would import everything at the start\n",
    "## but here, we will mport things as needed\n",
    "## to make their functions clear\n",
    "from tdc.single_pred import ADME # this is the dataset, \n",
    "                                 # from https://tdcommons.ai/single_pred_tasks/adme/#lipophilicity-astrazeneca \n",
    "## load the data \n",
    "data = ADME(name = 'Lipophilicity_AstraZeneca')\n",
    "## split into test and train sections\n",
    "split = data.get_split()\n",
    "# take a look at the data:\n",
    "print(\"split is data type %s \" % type(split))\n",
    "print(\"with keys: %s \" % list(split.keys()))\n",
    "print(split['train'].head())\n",
    "print(\"train size %i molecules\" % split['train'].shape[0])\n",
    "print(\"validation size %i molecules\" % split['valid'].shape[0])\n",
    "print(\"test size %i molecules\" % split['test'].shape[0])\n",
    "# map them for convenience\n",
    "train = split['train']\n",
    "test = split['valid'] # not going to use here\n",
    "val = split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1373bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's look at a molecule using RDkit\n",
    "from rdkit import Chem\n",
    "print(\"a SMILES string: %s\" %train[\"Drug\"].values[0])\n",
    "mol_object = Chem.MolFromSmiles(train[\"Drug\"].values[0])\n",
    "mol_object # you should see a picture of the molecule here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## in order to compute descriptors, we need to compute \n",
    "## mol objects for all strings\n",
    "for df in [train,test,val]:\n",
    "    df['mol'] = [Chem.MolFromSmiles(s) for s in df[\"Drug\"].values]\n",
    "print(train.head()) # note the exta colum above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51113b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1198c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## note the \"Y\" values - this is what we will try and predict\n",
    "## these numbers are lipophilicity measurements, roughly \n",
    "## how polar molecules are. The more positive the number,\n",
    "## the more nonpolar/fatty\n",
    "print(train[\"Y\"].head())\n",
    "## let's look at the two extremes \n",
    "from rdkit.Chem.Draw import MolsToGridImage \n",
    "MolsToGridImage(mols=train.loc[(train[\"Y\"]==min(train[\"Y\"])) | (train[\"Y\"]==max(train[\"Y\"])),\"mol\"],\n",
    "                molsPerRow = 2, legends = [\"Y = %.2f\" % y for y in train.loc[(train[\"Y\"]==min(train[\"Y\"])) | (train[\"Y\"]==max(train[\"Y\"])),\"Y\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we will calculate descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors # using rdkit\n",
    "names_of_descriptors = [x[0] for x in Chem.Descriptors._descList]\n",
    "print(\"we have %i dscriptors for each molecule\" % len(names_of_descriptors))\n",
    "print(\"first five desctripors are %s\" % names_of_descriptors[0:5])\n",
    "## you can find out about these descriptors here \n",
    "## https://www.rdkit.org/docs/source/rdkit.Chem.Descriptors.html\n",
    "## this calc object will be used to convert mol -> decsriptors\n",
    "calc = MoleculeDescriptors.MolecularDescriptorCalculator(names_of_descriptors)\n",
    "for df in [train,test,val]:  # computationally heavy step\n",
    "    df['descriptor_vector'] = [calc.CalcDescriptors(m) for m in df[\"mol\"].values]\n",
    "print(train.head()) # note the exta colum above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"here is one complete descriptor\")\n",
    "print(train[\"descriptor_vector\"].values[0]) # note the exta colum above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for easier use of sk learn I will convert descriptors to numpy\n",
    "import numpy as np\n",
    "train_descs = np.stack(train[\"descriptor_vector\"])\n",
    "test_descs = np.stack(test[\"descriptor_vector\"])\n",
    "val_descs = np.stack(val[\"descriptor_vector\"])\n",
    "## note these are only the descriptors!\n",
    "print(train_descs.shape)\n",
    "print(test_descs.shape)\n",
    "print(val_descs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale the data based on the train desc values\n",
    "from sklearn import preprocessing # you can look this function up in the sk docs \n",
    "x_train_scaler = preprocessing.StandardScaler() # create a scaler object\n",
    "x_train_scaler.fit_transform(train_descs) # fit the scaler on the train data only\n",
    "train_descs = x_train_scaler.transform(train_descs)\n",
    "test_descs = x_train_scaler.transform(test_descs)\n",
    "val_descs = x_train_scaler.transform(val_descs)\n",
    "## this is implementing\n",
    "## z' = (z - \\mu)/\\sigma,\n",
    "## i.e. subtract the mean and divide by the std dev\n",
    "## note that if you take the average now, it will be zero:\n",
    "## sum(z')/n = ((sum(z)-n*\\mu))/\\sigma*n)\n",
    "##           = ((n*\\mu-n*\\mu))/\\sigma*n)\n",
    "##           = 0 \n",
    "## similar argument implies the new std is one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ae916",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at some statisics \n",
    "print(np.mean(train_descs,0))\n",
    "## see how all the mean values are near zero and the \n",
    "## but !! look at the \"nan\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this occurs because the std dev (\\sigma) for some columns is zero\n",
    "## Why? because all molecules in the training set have the same value\n",
    "## this implies that \\sigma = 0 and then anything divided by zero =  nan\n",
    "## note that these \"constant\"/invariant columns don't tell us anything\n",
    "## since every molecule has the same value.Therefore, we will remove these\n",
    "## columns. Since we need to keep the size of the input to the model constant\n",
    "## we need to drop them for the tes/val set as well\n",
    "print(train_descs.shape) # just for reference\n",
    "drop_columns = np.isnan(np.sum(train_descs,0))\n",
    "train_descs = train_descs[:,~drop_columns] # [:, ~X] == [every row, NOT X columns]\n",
    "test_descs = test_descs[:,~drop_columns]\n",
    "val_descs = val_descs[:,~drop_columns]\n",
    "print(train_descs.shape) # after dropping columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e77338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the difference compared to the above:\n",
    "print(\"here is one complete descriptor AFTER scaling\")\n",
    "print(train_descs[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now at last we can fit a model. \n",
    "## Let us start simple with a multiple linear model (MLR)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression() # create the MLR object\n",
    "## here we run the model fitting\n",
    "linear_model.fit(train_descs,train[\"Y\"])\n",
    "## this is a linear modee\n",
    "print(\"The first 5 coefficients are: \")\n",
    "print(linear_model.coef_[0:5])\n",
    "print(\"The intercept is: \")\n",
    "print(linear_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48024a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now let us use our model to predict the labels\n",
    "## for all datasets\n",
    "linear_model_train_preds = linear_model.predict(train_descs)\n",
    "linear_model_test_preds = linear_model.predict(test_descs)\n",
    "linear_model_val_preds = linear_model.predict(val_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfabc9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's plot these and calc r2\n",
    "from sklearn.metrics import r2_score\n",
    "fig, (ax1, ax2, ax3)  = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True)\n",
    "ax1.scatter(train[\"Y\"],linear_model_train_preds,8,'b',label='linear',alpha=0.25)\n",
    "ax1.set_xlabel('real Y')\n",
    "ax1.set_ylabel('predicted Y')\n",
    "ax1.set_title(\"train data: r2 %.2f\" %r2_score(train[\"Y\"],linear_model_train_preds))\n",
    "ax1.plot(train[\"Y\"],np.poly1d(np.polyfit(train[\"Y\"],\n",
    "        linear_model_train_preds,1))(train[\"Y\"]),\n",
    "        \"k\",linewidth=2)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.scatter(val[\"Y\"],linear_model_val_preds,8,'orange',label='linear',alpha=0.75)\n",
    "ax2.set_xlabel('real Y')\n",
    "ax2.set_ylabel('predicted Y')\n",
    "ax2.set_title(\"val data: r2 %.2f\" %r2_score(val[\"Y\"],linear_model_val_preds))\n",
    "ax2.plot(val[\"Y\"],np.poly1d(np.polyfit(val[\"Y\"],\n",
    "        linear_model_val_preds,1))(val[\"Y\"]),\n",
    "        \"k\",linewidth=2)\n",
    "ax2.legend()\n",
    "\n",
    "ax3.scatter(test[\"Y\"],linear_model_test_preds,8,'r',label='linear',alpha=0.75)\n",
    "ax3.set_xlabel('real Y')\n",
    "ax3.set_ylabel('predicted Y')\n",
    "ax3.set_title(\"test data: r2 %.2f\" %r2_score(test[\"Y\"],linear_model_test_preds))\n",
    "ax3.plot(test[\"Y\"],np.poly1d(np.polyfit(test[\"Y\"],\n",
    "        linear_model_test_preds,1))(test[\"Y\"]),\n",
    "        \"k\",linewidth=2)\n",
    "ax3.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eefb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now let us try a ridge regression model\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_model = Ridge() # create the RR object\n",
    "## note that ridge depends (heavily!)\n",
    "## on the hyperparameter alpha \n",
    "print(\"the default value of alpha is %.2f\" % ridge_model.alpha)\n",
    "## note that in most texts, \"alpha\" is called \"lambda\" \n",
    "## it represents the bias-variance tradeoff.\n",
    "## understanding the implications of this parameter is \n",
    "## very important. To start to look into it,\n",
    "## let us fit a ride model with 2 different \n",
    "## values of alpha:\n",
    "ridge_model_a1 = Ridge(alpha = 1)\n",
    "ridge_model_a1.fit(train_descs,train[\"Y\"])\n",
    "ridge_a1_model_train_preds = ridge_model_a1.predict(train_descs)\n",
    "ridge_a1_model_test_preds = ridge_model_a1.predict(test_descs)\n",
    "ridge_a1_model_val_preds = ridge_model_a1.predict(val_descs)\n",
    "# now, again with a smaller alpha\n",
    "ridge_model_a1000 = Ridge(alpha = 1e4)\n",
    "ridge_model_a1000.fit(train_descs,train[\"Y\"])\n",
    "ridge_a1000_model_train_preds = ridge_model_a1000.predict(train_descs)\n",
    "ridge_a1000_model_test_preds = ridge_model_a1000.predict(test_descs)\n",
    "ridge_a1000_model_val_preds = ridge_model_a1000.predict(val_descs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e980ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's plot and compare\n",
    "from sklearn.metrics import r2_score\n",
    "fig, ((ax11, ax12, ax13), (ax21, ax22, ax23))  = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True) \n",
    "ax11.scatter(train[\"Y\"],ridge_a1_model_train_preds,8,'b',label='ridge, a=10',alpha=0.25)\n",
    "ax11.set_xlabel('real Y')\n",
    "ax11.set_ylabel('predicted Y')\n",
    "ax11.set_title(\"train data: r2 %.2f\" %r2_score(train[\"Y\"],ridge_a1_model_train_preds))\n",
    "ax11.plot(train[\"Y\"],np.poly1d(np.polyfit(train[\"Y\"],\n",
    "        ridge_a1_model_train_preds,1))(train[\"Y\"]),\n",
    "        \"k\",linewidth=2)\n",
    "ax11.legend()\n",
    "ax21.scatter(train[\"Y\"],ridge_a1000_model_train_preds,8,'b',label='ridge, a=0.1',alpha=0.25)\n",
    "ax21.set_xlabel('real Y')\n",
    "ax21.set_ylabel('predicted Y')\n",
    "ax21.set_title(\"train data: r2 %.2f\" %r2_score(train[\"Y\"],ridge_a1000_model_train_preds))\n",
    "ax21.plot(train[\"Y\"],np.poly1d(np.polyfit(train[\"Y\"],\n",
    "        ridge_a1000_model_train_preds,1))(train[\"Y\"]),\n",
    "        \"k\",linewidth=2)\n",
    "ax21.legend()\n",
    "\n",
    "\n",
    "ax12.scatter(val[\"Y\"],ridge_a1_model_val_preds,8,'orange',label='ridge, a=10',alpha=0.75)\n",
    "ax12.set_xlabel('real Y')\n",
    "ax12.set_ylabel('predicted Y')\n",
    "ax12.set_title(\"val data: r2 %.2f\" %r2_score(val[\"Y\"],ridge_a1_model_val_preds))\n",
    "ax12.plot(val[\"Y\"],np.poly1d(np.polyfit(val[\"Y\"],\n",
    "        ridge_a1_model_val_preds,1))(val[\"Y\"]),\n",
    "        \"k\",linewidth=2)\n",
    "ax12.legend()\n",
    "\n",
    "ax13.scatter(test[\"Y\"],ridge_a1_model_test_preds,8,'r',label='ridge, a=10',alpha=0.75)\n",
    "ax13.set_xlabel('real Y')\n",
    "ax13.set_ylabel('predicted Y')\n",
    "ax13.set_title(\"test data: r2 %.2f\" %r2_score(test[\"Y\"],ridge_a1_model_test_preds))\n",
    "ax13.plot(test[\"Y\"],np.poly1d(np.polyfit(test[\"Y\"],\n",
    "        ridge_a1_model_test_preds,1))(test[\"Y\"]),\n",
    "        \"k\",linewidth=2)\n",
    "ax13.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True)\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
